{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import pickle\n",
    "\n",
    "import keras\n",
    "import keras.backend as K\n",
    "from keras.optimizers import Adam\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Activation, Input, LSTM\n",
    "\n",
    "from ptbdb_cache import load_records"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3, 120012, 15) (3, 1)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(385, 82, 82, 549)"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "from random import shuffle\n",
    "\n",
    "from keras.utils import Sequence\n",
    "\n",
    "DATA_DIRECTORY = 'data/cached_records'\n",
    "MAX_LENGTH = 120012\n",
    "NUM_CHANNELS = 15\n",
    "\n",
    "\n",
    "def pad_sequence(sequence, max_len=MAX_LENGTH):\n",
    "    pad_width = ((0,max_len - sequence.shape[0]), (0, 0))\n",
    "    \n",
    "    return np.pad(sequence, pad_width, mode='constant', constant_values=0)\n",
    "\n",
    "\n",
    "def get_train_dev_test_filenames(fraction=0.15):\n",
    "    ptbdb_filenames = os.listdir(DATA_DIRECTORY)\n",
    "    \n",
    "    shuffle(ptbdb_filenames)\n",
    "    \n",
    "    n_holdouts = int(fraction * len(ptbdb_filenames))\n",
    "    \n",
    "    train = ptbdb_filenames[:-2 * n_holdouts]\n",
    "    dev = ptbdb_filenames[-2 * n_holdouts: -n_holdouts]\n",
    "    test = ptbdb_filenames[-n_holdouts:]\n",
    "    \n",
    "    return train, dev, test\n",
    "\n",
    "\n",
    "class CacheBatchGenerator(Sequence):\n",
    "\n",
    "    def __init__(self, filenames, batch_size, basedir=DATA_DIRECTORY):\n",
    "        self.filenames = filenames\n",
    "        self.batch_size = batch_size\n",
    "        self.basedir = basedir\n",
    "\n",
    "    def __len__(self):\n",
    "        return int(np.ceil(len(self.filenames) / float(self.batch_size)))\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        batch_filenames = self.filenames[idx * self.batch_size:(idx + 1) * self.batch_size]\n",
    "        \n",
    "        batch = []\n",
    "        for filename in batch_filenames:\n",
    "            \n",
    "            with open(self.basedir + '/' + filename, 'rb') as f:\n",
    "                data = pickle.load(f)\n",
    "                \n",
    "            batch.append(data)\n",
    "        \n",
    "        batch_x, batch_y = zip(*batch)\n",
    "        \n",
    "        batch_x = [pad_sequence(x) for x in batch_x]\n",
    "        batch_x = np.stack(batch_x)\n",
    "        \n",
    "        \n",
    "        batch_y = [1 if r == 'Myocardial infarction' else 0 for r in batch_y]\n",
    "        batch_y = np.array(batch_y).reshape(-1, 1)\n",
    "        \n",
    "        return batch_x, batch_y\n",
    "    \n",
    "\n",
    "seq = CacheBatchGenerator(ptbdb_filenames, 3)\n",
    "first = seq[1]\n",
    "print(first[0].shape, first[1].shape)\n",
    "\n",
    "trainf, devf, testf = get_train_dev_test_filenames()\n",
    "\n",
    "len(CacheBatchGenerator(trainf, 3))\n",
    "\n",
    "len(trainf), len(devf), len(testf), sum([len(trainf), len(devf), len(testf)])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "numpy.int64"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(first[1][0][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def f1_score(y_true, y_pred):\n",
    "\n",
    "    # Count positive samples.\n",
    "    c1 = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n",
    "    c2 = K.sum(K.round(K.clip(y_pred, 0, 1)))\n",
    "    c3 = K.sum(K.round(K.clip(y_true, 0, 1)))\n",
    "\n",
    "    # If there are no true samples, fix the F1 score at 0.\n",
    "    if c3 == 0:\n",
    "        return 0\n",
    "\n",
    "    # How many selected items are relevant?\n",
    "    precision = c1 / c2\n",
    "\n",
    "    # How many relevant items are selected?\n",
    "    recall = c1 / c3\n",
    "\n",
    "    # Calculate f1_score\n",
    "    f1_score = 2 * (precision * recall) / (precision + recall)\n",
    "    return f1_score "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'batch_size': 135,\n",
       " 'learning_rate': 0.0001133999064787136,\n",
       " 'num_hidden_units': 94}"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def get_random_hyperparameters():\n",
    "    train_batch, _, _ = get_train_dev_test_filenames()\n",
    "    \n",
    "    return {\n",
    "        'num_hidden_units': np.random.randint(4, 100),\n",
    "        'batch_size': np.random.randint(10, int(len(train_batch) / 2)),\n",
    "        'learning_rate': 10**(-4 * np.random.random())\n",
    "    }\n",
    "\n",
    "get_random_hyperparameters()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "metrics to collect: \n",
    "- history (with loss, accuracy, f1 score)\n",
    "- time to run 10 epochs\n",
    "- metrics on dev set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_model_with_random_hyperparameters(n_epochs=5):\n",
    "    results = {}\n",
    "    \n",
    "    hyperparameters = get_random_hyperparameters()\n",
    "    print('trying parameters: {}'.format(hyperparameters))\n",
    "    results['hyperparameters'] = hyperparameters\n",
    "    \n",
    "    batch_size = hyperparameters['batch_size']\n",
    "    num_hidden_units = hyperparameters['num_hidden_units']\n",
    "    learning_rate = hyperparameters['learning_rate']\n",
    "    \n",
    "    # Build model\n",
    "    optimizer = Adam(lr=learning_rate)\n",
    "    \n",
    "    model = Sequential([\n",
    "        LSTM(num_hidden_units, input_shape=(MAX_LENGTH, NUM_CHANNELS)),\n",
    "        Dense(1, activation='sigmoid'),\n",
    "    ])\n",
    "\n",
    "    model.compile(optimizer=optimizer,\n",
    "                  loss='binary_crossentropy',\n",
    "                  metrics=['accuracy', f1_score])\n",
    "    \n",
    "    results['model'] = model\n",
    "    \n",
    "    # Train model\n",
    "    t_before = time.time()\n",
    "    \n",
    "    train_files, dev_files, _ = get_train_dev_test_filenames()\n",
    "    \n",
    "    training_batch_generator = CacheBatchGenerator(train_files, batch_size=batch_size)\n",
    "    dev_batch_generator = CacheBatchGenerator(dev_files, batch_size=batch_size)\n",
    "    \n",
    "    history = model.fit_generator(\n",
    "        generator=training_batch_generator,\n",
    "        steps_per_epoch=(len(train_files) // batch_size),\n",
    "        validation_data=dev_batch_generator,\n",
    "        validation_steps=(len(dev_files) // batch_size),\n",
    "        epochs=n_epochs,\n",
    "        use_multiprocessing=True,\n",
    "        workers=16,\n",
    "        max_queue_size=32,\n",
    "        verbose=2)\n",
    "    \n",
    "    results['history'] = history\n",
    "    \n",
    "    t_after = time.time()\n",
    "    total_time = t_after - t_before\n",
    "    results['training_time'] = total_time\n",
    "    \n",
    "    # Test model\n",
    "    training_batch_generator = CacheBatchGenerator(train_files, batch_size=batch_size)\n",
    "    dev_batch_generator = CacheBatchGenerator(dev_files, batch_size=batch_size)\n",
    "    \n",
    "    results['train_metrics'] = model.evaluate_generator(training_batch_generator)\n",
    "    results['dev_metrics'] = model.evaluate_generator(dev_batch_generator)\n",
    "    \n",
    "    # Print results\n",
    "    print('metrics names: ', model.metrics_names)\n",
    "    print('train_scores: ', results['train_metrics'])\n",
    "    print('dev_scores: ', results['dev_metrics'])\n",
    "   \n",
    "    plt.figure()\n",
    "    plt.plot(history.history['loss'])\n",
    "    plt.title(hyperparameters)\n",
    "    plt.show()\n",
    "    \n",
    "    return results\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "testing model 1 of 5\n",
      "trying parameters: {'num_hidden_units': 32, 'batch_size': 144, 'learning_rate': 0.009570521618064822}\n",
      "Epoch 1/5\n"
     ]
    }
   ],
   "source": [
    "N_MODELS = 5\n",
    "N_EPOCHS = 5\n",
    "\n",
    "model_test_results = []\n",
    "for i in range(N_MODELS):\n",
    "    print('\\ntesting model {} of {}'.format(i + 1, N_MODELS))\n",
    "    \n",
    "    try:\n",
    "        model_result = run_model_with_random_hyperparameters(n_epochs=N_EPOCHS)\n",
    "\n",
    "        model_test_results.append(model_result)\n",
    "    except KeyboardInterrupt:\n",
    "        print('Test Interrupted.')\n",
    "        break\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
